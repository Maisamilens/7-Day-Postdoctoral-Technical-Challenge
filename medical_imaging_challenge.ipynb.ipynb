{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":23812,"sourceType":"datasetVersion","datasetId":17810}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfrom pathlib import Path\nfrom PIL import Image\nimport pandas as pd\nfrom tqdm import tqdm\n\n# Base path\nbase_path = Path(\"/kaggle/input/chest-xray-pneumonia/chest_xray\")\n\nsplits = [\"train\", \"val\", \"test\"]\n\ndata = []\n\nprint(\"Scanning dataset...\")\n\nfor split in splits:\n    split_path = base_path / split\n    \n    for cls in split_path.iterdir():\n        if cls.is_dir():\n            \n            for img_path in tqdm(list(cls.glob(\"*\")), desc=f\"{split}-{cls.name}\"):\n                try:\n                    # File size in KB\n                    file_size_kb = img_path.stat().st_size / 1024\n                    \n                    # Open image\n                    with Image.open(img_path) as img:\n                        width, height = img.size\n                        img_format = img.format\n                        \n                    data.append({\n                        \"split\": split,\n                        \"class\": cls.name,\n                        \"format\": img_format,\n                        \"width\": width,\n                        \"height\": height,\n                        \"file_size_kb\": file_size_kb\n                    })\n                    \n                except Exception as e:\n                    print(f\"Error reading {img_path}: {e}\")\n\n# Create dataframe\ndf = pd.DataFrame(data)\n\nprint(\"\\nTotal Images Scanned:\", len(df))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T09:52:43.758548Z","iopub.execute_input":"2026-02-18T09:52:43.758862Z","iopub.status.idle":"2026-02-18T09:54:03.586181Z","shell.execute_reply.started":"2026-02-18T09:52:43.758837Z","shell.execute_reply":"2026-02-18T09:54:03.585488Z"}},"outputs":[{"name":"stdout","text":"Scanning dataset...\n","output_type":"stream"},{"name":"stderr","text":"train-PNEUMONIA: 100%|██████████| 3875/3875 [00:42<00:00, 90.42it/s] \ntrain-NORMAL: 100%|██████████| 1341/1341 [00:28<00:00, 46.44it/s]\nval-PNEUMONIA: 100%|██████████| 8/8 [00:00<00:00, 122.02it/s]\nval-NORMAL: 100%|██████████| 8/8 [00:00<00:00, 101.83it/s]\ntest-PNEUMONIA: 100%|██████████| 390/390 [00:04<00:00, 94.36it/s] \ntest-NORMAL: 100%|██████████| 234/234 [00:03<00:00, 63.29it/s]","output_type":"stream"},{"name":"stdout","text":"\nTotal Images Scanned: 5856\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\"\"\"\nTask 1: CNN Classification with Comprehensive Analysis\n=======================================================\nChest X-Ray Pneumonia Detection using EfficientNet-B3\nKaggle Environment: /kaggle/input/chest-xray-pneumonia/chest_xray/\n\nAuthor: Postdoctoral Challenge Submission\nDataset: chest-xray-pneumonia (Kaggle)\n\"\"\"\n\nimport os\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.models import EfficientNet_B3_Weights\n\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, confusion_matrix, roc_curve, classification_report\n)\n\n# ─────────────────────────────────────────────\n# Fix for PyTorch ≥ 2.4 / 2.5 weights_only=True default\n# ─────────────────────────────────────────────\nimport torch.serialization\ntorch.serialization.add_safe_globals([np._core.multiarray.scalar])\n\nwarnings.filterwarnings('ignore')\n\n# ─────────────────────────────────────────────\n#  CONFIG\n# ─────────────────────────────────────────────\nclass Config:\n    DATA_ROOT   = Path(\"/kaggle/input/chest-xray-pneumonia/chest_xray\")\n    OUTPUT_DIR  = Path(\"/kaggle/working/task1_outputs\")\n    MODEL_DIR   = Path(\"/kaggle/working/models\")\n\n    IMAGE_SIZE  = 224\n    BATCH_SIZE  = 32\n    NUM_EPOCHS  = 25\n    LR          = 1e-4\n    WEIGHT_DECAY= 1e-5\n    NUM_WORKERS = 2\n    SEED        = 42\n\n    CLASSES     = ['NORMAL', 'PNEUMONIA']\n    NUM_CLASSES = 2\n\n\ndef seed_everything(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef setup_dirs():\n    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    Config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n\n\n# ─────────────────────────────────────────────\n#  DATASET\n# ─────────────────────────────────────────────\nclass ChestXRayDataset(Dataset):\n    def __init__(self, root: Path, split: str, transform=None):\n        self.transform = transform\n        self.samples   = []\n        self.labels    = []\n\n        for label_idx, cls in enumerate(Config.CLASSES):\n            cls_dir = root / split / cls\n            if not cls_dir.exists():\n                print(f\"[WARN] {cls_dir} not found, skipping.\")\n                continue\n            for img_path in sorted(cls_dir.glob(\"*.jpeg\")):\n                self.samples.append(img_path)\n                self.labels.append(label_idx)\n\n        print(f\"[{split.upper()}] Loaded {len(self.samples)} images | \"\n              f\"NORMAL={self.labels.count(0)} | PNEUMONIA={self.labels.count(1)}\")\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        img   = Image.open(self.samples[idx]).convert(\"RGB\")\n        label = self.labels[idx]\n        if self.transform:\n            img = self.transform(img)\n        return img, label\n\n\ndef get_transforms():\n    mean = [0.485, 0.456, 0.406]\n    std  = [0.229, 0.224, 0.225]\n\n    train_tf = transforms.Compose([\n        transforms.Resize((Config.IMAGE_SIZE + 32, Config.IMAGE_SIZE + 32)),\n        transforms.RandomCrop(Config.IMAGE_SIZE),\n        transforms.RandomHorizontalFlip(p=0.5),\n        transforms.RandomRotation(degrees=10),\n        transforms.ColorJitter(brightness=0.2, contrast=0.2),\n        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    eval_tf = transforms.Compose([\n        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std),\n    ])\n\n    return train_tf, eval_tf\n\n\ndef get_dataloaders():\n    train_tf, eval_tf = get_transforms()\n\n    train_ds = ChestXRayDataset(Config.DATA_ROOT, \"train\", train_tf)\n    val_ds   = ChestXRayDataset(Config.DATA_ROOT, \"val\",   eval_tf)\n    test_ds  = ChestXRayDataset(Config.DATA_ROOT, \"test\",  eval_tf)\n\n    labels  = np.array(train_ds.labels)\n    counts  = np.bincount(labels)\n    weights = 1.0 / counts[labels]\n    sampler = torch.utils.data.WeightedRandomSampler(\n        weights=torch.DoubleTensor(weights),\n        num_samples=len(labels),\n        replacement=True\n    )\n\n    train_loader = DataLoader(train_ds, batch_size=Config.BATCH_SIZE,\n                              sampler=sampler, num_workers=Config.NUM_WORKERS,\n                              pin_memory=True)\n    val_loader   = DataLoader(val_ds,   batch_size=Config.BATCH_SIZE,\n                              shuffle=False, num_workers=Config.NUM_WORKERS)\n    test_loader  = DataLoader(test_ds,  batch_size=Config.BATCH_SIZE,\n                              shuffle=False, num_workers=Config.NUM_WORKERS)\n\n    return train_loader, val_loader, test_loader, test_ds\n\n\n# ─────────────────────────────────────────────\n#  MODEL\n# ─────────────────────────────────────────────\ndef build_model(device):\n    model = models.efficientnet_b3(weights=EfficientNet_B3_Weights.IMAGENET1K_V1)\n\n    for param in model.parameters():\n        param.requires_grad = False\n\n    for name, param in model.named_parameters():\n        if \"features.7\" in name or \"features.8\" in name:\n            param.requires_grad = True\n\n    in_features = model.classifier[1].in_features\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.4, inplace=True),\n        nn.Linear(in_features, 256),\n        nn.SiLU(),\n        nn.Dropout(p=0.2),\n        nn.Linear(256, Config.NUM_CLASSES)\n    )\n\n    model = model.to(device)\n    total_params     = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"\\nModel: EfficientNet-B3\")\n    print(f\"  Total params    : {total_params:,}\")\n    print(f\"  Trainable params: {trainable_params:,}\")\n    return model\n\n\n# ─────────────────────────────────────────────\n#  TRAINING\n# ─────────────────────────────────────────────\ndef train_one_epoch(model, loader, criterion, optimizer, device, scaler):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n\n    for imgs, labels in tqdm(loader, desc=\"  Train\", leave=False):\n        imgs, labels = imgs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n            outputs = model(imgs)\n            loss    = criterion(outputs, labels)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * imgs.size(0)\n        preds         = outputs.argmax(dim=1)\n        correct      += (preds == labels).sum().item()\n        total        += imgs.size(0)\n\n    return running_loss / total, correct / total\n\n\n@torch.no_grad()\ndef evaluate(model, loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    all_preds, all_labels, all_probs = [], [], []\n\n    for imgs, labels in loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        outputs = model(imgs)\n        loss    = criterion(outputs, labels)\n\n        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n        preds = outputs.argmax(dim=1).cpu().numpy()\n        lbls  = labels.cpu().numpy()\n\n        running_loss += loss.item() * imgs.size(0)\n        correct      += (preds == lbls).sum()\n        total        += len(lbls)\n\n        all_preds.extend(preds)\n        all_labels.extend(lbls)\n        all_probs.extend(probs)\n\n    metrics = compute_metrics(all_labels, all_preds, all_probs)\n    metrics['loss'] = running_loss / total\n    return metrics, all_preds, all_labels, all_probs\n\n\ndef compute_metrics(labels, preds, probs):\n    return {\n        'accuracy' : accuracy_score(labels, preds),\n        'precision': precision_score(labels, preds, zero_division=0),\n        'recall'   : recall_score(labels, preds, zero_division=0),\n        'f1'       : f1_score(labels, preds, zero_division=0),\n        'auc'      : roc_auc_score(labels, probs),\n    }\n\n\ndef train(model, train_loader, val_loader, device):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.AdamW(\n        filter(lambda p: p.requires_grad, model.parameters()),\n        lr=Config.LR, weight_decay=Config.WEIGHT_DECAY\n    )\n    scheduler = CosineAnnealingLR(optimizer, T_max=Config.NUM_EPOCHS, eta_min=1e-6)\n    scaler    = torch.cuda.amp.GradScaler()\n\n    history = {k: [] for k in [\n        'train_loss','train_acc','val_loss','val_acc',\n        'val_f1','val_auc','val_precision','val_recall'\n    ]}\n    best_val_auc = 0.0\n    best_model_path = Config.MODEL_DIR / \"best_efficientnet_b3.pth\"\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"TRAINING\")\n    print(\"=\"*60)\n\n    for epoch in range(1, Config.NUM_EPOCHS + 1):\n        t_loss, t_acc = train_one_epoch(model, train_loader, criterion,\n                                        optimizer, device, scaler)\n        v_metrics, _, _, _ = evaluate(model, val_loader, criterion, device)\n        scheduler.step()\n\n        history['train_loss'].append(t_loss)\n        history['train_acc'].append(t_acc)\n        history['val_loss'].append(v_metrics['loss'])\n        history['val_acc'].append(v_metrics['accuracy'])\n        history['val_f1'].append(v_metrics['f1'])\n        history['val_auc'].append(v_metrics['auc'])\n        history['val_precision'].append(v_metrics['precision'])\n        history['val_recall'].append(v_metrics['recall'])\n\n        if v_metrics['auc'] > best_val_auc:\n            best_val_auc = v_metrics['auc']\n            torch.save({\n                'epoch'               : epoch,\n                'model_state_dict'    : model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'val_auc'             : float(best_val_auc),   # ← important: force python float\n            }, best_model_path)\n\n        print(f\"Epoch [{epoch:02d}/{Config.NUM_EPOCHS}] | \"\n              f\"T-Loss: {t_loss:.4f} T-Acc: {t_acc:.4f} | \"\n              f\"V-Loss: {v_metrics['loss']:.4f} V-Acc: {v_metrics['accuracy']:.4f} | \"\n              f\"V-F1: {v_metrics['f1']:.4f} V-AUC: {v_metrics['auc']:.4f} \"\n              f\"{'★ BEST' if v_metrics['auc'] > best_val_auc - 1e-6 else ''}\")\n\n    print(f\"\\nBest Val AUC: {best_val_auc:.4f}\")\n    print(f\"Saved to: {best_model_path}\")\n    return history, best_model_path\n\n\n# ─────────────────────────────────────────────\n#  VISUALIZATIONS  (unchanged except minor cleanup)\n# ─────────────────────────────────────────────\ndef plot_training_curves(history):\n    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n    fig.suptitle('Training History – EfficientNet-B3', fontsize=16, fontweight='bold')\n\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    axes[0,0].plot(epochs, history['train_loss'], 'b-o', ms=4, label='Train')\n    axes[0,0].plot(epochs, history['val_loss'],   'r-o', ms=4, label='Val')\n    axes[0,0].set_title('Loss'); axes[0,0].legend(); axes[0,0].set_xlabel('Epoch')\n\n    axes[0,1].plot(epochs, history['train_acc'], 'b-o', ms=4, label='Train')\n    axes[0,1].plot(epochs, history['val_acc'],   'r-o', ms=4, label='Val')\n    axes[0,1].set_title('Accuracy'); axes[0,1].legend(); axes[0,1].set_xlabel('Epoch')\n\n    axes[0,2].plot(epochs, history['val_auc'], 'g-o', ms=4)\n    axes[0,2].set_title('Val AUC'); axes[0,2].set_xlabel('Epoch')\n\n    axes[1,0].plot(epochs, history['val_f1'], 'm-o', ms=4)\n    axes[1,0].set_title('Val F1'); axes[1,0].set_xlabel('Epoch')\n\n    axes[1,1].plot(epochs, history['val_precision'], 'c-o', ms=4, label='Precision')\n    axes[1,1].plot(epochs, history['val_recall'],    'y-o', ms=4, label='Recall')\n    axes[1,1].set_title('Precision & Recall'); axes[1,1].legend(); axes[1,1].set_xlabel('Epoch')\n\n    axes[1,2].text(0.5, 0.5,\n        f\"Best Val AUC\\n{max(history['val_auc']):.4f}\\n\\n\"\n        f\"Best Val F1\\n{max(history['val_f1']):.4f}\",\n        ha='center', va='center', fontsize=14,\n        transform=axes[1,2].transAxes)\n    axes[1,2].set_title('Summary')\n    axes[1,2].axis('off')\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"training_curves.png\"\n    plt.savefig(path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path}\")\n\n\ndef plot_confusion_matrix(labels, preds):\n    cm   = confusion_matrix(labels, preds)\n    cmn  = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    fig.suptitle('Confusion Matrix – Test Set', fontsize=14, fontweight='bold')\n\n    for ax, data, fmt, title in zip(\n        axes, [cm, cmn], ['d', '.2%'], ['Raw Counts', 'Normalized']\n    ):\n        sns.heatmap(data, annot=True, fmt=fmt, cmap='Blues', ax=ax,\n                    xticklabels=Config.CLASSES, yticklabels=Config.CLASSES,\n                    linewidths=0.5, cbar=True)\n        ax.set_title(title, fontsize=12)\n        ax.set_xlabel('Predicted', fontsize=11)\n        ax.set_ylabel('True', fontsize=11)\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"confusion_matrix.png\"\n    plt.savefig(path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path}\")\n    return cm\n\n\ndef plot_roc_curve(labels, probs):\n    fpr, tpr, thresholds = roc_curve(labels, probs)\n    auc = roc_auc_score(labels, probs)\n\n    j_scores = tpr - fpr\n    opt_idx  = np.argmax(j_scores)\n    opt_thr  = thresholds[opt_idx]\n\n    fig, ax = plt.subplots(figsize=(8, 7))\n    ax.plot(fpr, tpr, 'b-', lw=2, label=f'EfficientNet-B3 (AUC = {auc:.4f})')\n    ax.plot([0,1],[0,1], 'k--', lw=1, label='Random')\n    ax.scatter(fpr[opt_idx], tpr[opt_idx], color='red', s=120, zorder=5,\n               label=f'Optimal threshold = {opt_thr:.3f}')\n    ax.fill_between(fpr, tpr, alpha=0.10, color='steelblue')\n    ax.set_xlabel('False Positive Rate', fontsize=12)\n    ax.set_ylabel('True Positive Rate', fontsize=12)\n    ax.set_title('ROC Curve – Pneumonia Detection', fontsize=14, fontweight='bold')\n    ax.legend(fontsize=11)\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"roc_curve.png\"\n    plt.savefig(path, dpi=150, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path}\")\n    return opt_thr\n\n\ndef plot_sample_predictions(model, test_ds, device, n=16):\n    model.eval()\n    eval_tf = transforms.Compose([\n        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n\n    indices = random.sample(range(len(test_ds)), n)\n    fig, axes = plt.subplots(4, 4, figsize=(16, 16))\n    fig.suptitle('Sample Test Predictions', fontsize=16, fontweight='bold')\n\n    for ax, idx in zip(axes.ravel(), indices):\n        img_path = test_ds.samples[idx]\n        true_lbl = test_ds.labels[idx]\n\n        img_raw = Image.open(img_path).convert(\"RGB\")\n        img_t   = eval_tf(img_raw).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            out   = model(img_t)\n            prob  = torch.softmax(out, dim=1)[0,1].item()\n            pred  = out.argmax(dim=1).item()\n\n        correct = (pred == true_lbl)\n        color   = 'green' if correct else 'red'\n\n        ax.imshow(img_raw, cmap='gray', aspect='auto')\n        ax.set_title(\n            f\"True: {Config.CLASSES[true_lbl]}\\n\"\n            f\"Pred: {Config.CLASSES[pred]} ({prob:.2f})\",\n            fontsize=8, color=color, fontweight='bold'\n        )\n        ax.axis('off')\n        for spine in ax.spines.values():\n            spine.set_edgecolor(color)\n            spine.set_linewidth(3)\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"sample_predictions.png\"\n    plt.savefig(path, dpi=130, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path}\")\n\n\ndef plot_failure_cases(model, test_ds, preds, labels, device, n=12):\n    failures = [i for i,(p,l) in enumerate(zip(preds, labels)) if p != l]\n    if not failures:\n        print(\"No failures found (perfect model?)\")\n        return\n\n    n = min(n, len(failures))\n    selected = random.sample(failures, n)\n\n    eval_tf = transforms.Compose([\n        transforms.Resize((Config.IMAGE_SIZE, Config.IMAGE_SIZE)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n    ])\n\n    rows = (n + 3) // 4\n    fig, axes = plt.subplots(rows, 4, figsize=(16, 4*rows))\n    fig.suptitle('Failure Cases (Misclassified Images)', fontsize=16, fontweight='bold')\n    axes = axes.ravel() if n > 4 else axes.ravel()\n\n    model.eval()\n    for ax, idx in zip(axes, selected):\n        img_path = test_ds.samples[idx]\n        true_lbl = test_ds.labels[idx]\n\n        img_raw = Image.open(img_path).convert(\"RGB\")\n        img_t   = eval_tf(img_raw).unsqueeze(0).to(device)\n\n        with torch.no_grad():\n            out   = model(img_t)\n            prob  = torch.softmax(out, dim=1)[0,1].item()\n            pred  = out.argmax(dim=1).item()\n\n        ax.imshow(img_raw, cmap='gray')\n        ax.set_title(\n            f\"True: {Config.CLASSES[true_lbl]}\\n\"\n            f\"Pred: {Config.CLASSES[pred]} (conf:{prob:.2f})\",\n            fontsize=9, color='red', fontweight='bold'\n        )\n        ax.axis('off')\n\n    for ax in axes[len(selected):]:\n        ax.axis('off')\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"failure_cases.png\"\n    plt.savefig(path, dpi=130, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path} | Total failures: {len(failures)}\")\n\n\n# ─────────────────────────────────────────────\n#  REPORT  (unchanged)\n# ─────────────────────────────────────────────\ndef generate_report(metrics, cm, opt_threshold, history):\n    report_path = Config.OUTPUT_DIR / \"task1_classification_report.md\"\n\n    tn, fp, fn, tp = cm.ravel()\n    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0\n\n    md = f\"\"\"# Task 1: CNN Classification Report\n**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**Model:** EfficientNet-B3 (Transfer Learning)\n**Dataset:** Chest X-Ray Pneumonia (Kaggle)\n\n---\n\n## 1. Model Architecture\n...\n\n(keeping your original report content – omitted here for brevity)\n\"\"\"\n    # ← your full markdown content here (same as before)\n\n    report_path.write_text(md)\n    print(f\"\\nReport saved: {report_path}\")\n    return report_path\n\n\n# ─────────────────────────────────────────────\n#  MAIN\n# ─────────────────────────────────────────────\ndef main():\n    seed_everything(Config.SEED)\n    setup_dirs()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n\n    print(\"\\n── Loading Data ──\")\n    train_loader, val_loader, test_loader, test_ds = get_dataloaders()\n\n    print(\"\\n── Building Model ──\")\n    model = build_model(device)\n\n    print(\"\\n\" + \"=\"*60)\n    history, best_path = train(model, train_loader, val_loader, device)\n\n    print(\"\\n── Loading Best Model & Evaluating on Test Set ──\")\n    ckpt = torch.load(best_path, map_location=device)           # ← now safe\n    model.load_state_dict(ckpt['model_state_dict'])\n\n    criterion = nn.CrossEntropyLoss()\n    test_metrics, preds, labels, probs = evaluate(model, test_loader, criterion, device)\n\n    print(\"\\n\" + \"=\"*50)\n    print(\"TEST SET RESULTS\")\n    print(\"=\"*50)\n    for k, v in test_metrics.items():\n        print(f\"  {k:<12}: {v:.4f}\")\n\n    print(\"\\n\" + classification_report(labels, preds, target_names=Config.CLASSES))\n\n    print(\"\\n── Generating Visualizations ──\")\n    plot_training_curves(history)\n    cm = plot_confusion_matrix(labels, preds)\n    opt_thr = plot_roc_curve(labels, probs)\n    plot_sample_predictions(model, test_ds, device)\n    plot_failure_cases(model, test_ds, preds, labels, device)\n\n    print(\"\\n── Writing Report ──\")\n    generate_report(test_metrics, cm, opt_thr, history)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"TASK 1 COMPLETE\")\n    print(f\"All outputs: {Config.OUTPUT_DIR}\")\n    print(\"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T14:11:02.553628Z","iopub.execute_input":"2026-02-18T14:11:02.554261Z","iopub.status.idle":"2026-02-18T14:37:15.853784Z","shell.execute_reply.started":"2026-02-18T14:11:02.554216Z","shell.execute_reply":"2026-02-18T14:37:15.853049Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n\n── Loading Data ──\n[TRAIN] Loaded 5216 images | NORMAL=1341 | PNEUMONIA=3875\n[VAL] Loaded 16 images | NORMAL=8 | PNEUMONIA=8\n[TEST] Loaded 624 images | NORMAL=234 | PNEUMONIA=390\n\n── Building Model ──\nDownloading: \"https://download.pytorch.org/models/efficientnet_b3_rwightman-b3899882.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b3_rwightman-b3899882.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 47.2M/47.2M [00:00<00:00, 169MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\nModel: EfficientNet-B3\n  Total params    : 11,090,218\n  Trainable params: 4,271,100\n\n============================================================\n\n============================================================\nTRAINING\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [01/25] | T-Loss: 0.3031 T-Acc: 0.8854 | V-Loss: 0.2867 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9688 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [02/25] | T-Loss: 0.1465 T-Acc: 0.9469 | V-Loss: 0.2305 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9688 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [03/25] | T-Loss: 0.1178 T-Acc: 0.9532 | V-Loss: 0.4606 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9375 \n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [04/25] | T-Loss: 0.1158 T-Acc: 0.9595 | V-Loss: 0.3331 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9688 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [05/25] | T-Loss: 0.1201 T-Acc: 0.9540 | V-Loss: 0.2420 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9531 \n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [06/25] | T-Loss: 0.1016 T-Acc: 0.9588 | V-Loss: 0.2826 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9844 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [07/25] | T-Loss: 0.1032 T-Acc: 0.9638 | V-Loss: 0.2160 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9844 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [08/25] | T-Loss: 0.0922 T-Acc: 0.9653 | V-Loss: 0.2867 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 0.9844 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [09/25] | T-Loss: 0.0805 T-Acc: 0.9688 | V-Loss: 0.2223 V-Acc: 0.9375 | V-F1: 0.9333 V-AUC: 0.9688 \n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [10/25] | T-Loss: 0.0924 T-Acc: 0.9657 | V-Loss: 0.2143 V-Acc: 0.9375 | V-F1: 0.9333 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [11/25] | T-Loss: 0.0814 T-Acc: 0.9701 | V-Loss: 0.1850 V-Acc: 0.9375 | V-F1: 0.9333 V-AUC: 0.9844 \n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [12/25] | T-Loss: 0.0886 T-Acc: 0.9695 | V-Loss: 0.2416 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [13/25] | T-Loss: 0.0774 T-Acc: 0.9724 | V-Loss: 0.2221 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [14/25] | T-Loss: 0.0831 T-Acc: 0.9712 | V-Loss: 0.1816 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [15/25] | T-Loss: 0.0860 T-Acc: 0.9691 | V-Loss: 0.2608 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [16/25] | T-Loss: 0.0866 T-Acc: 0.9680 | V-Loss: 0.1532 V-Acc: 0.9375 | V-F1: 0.9333 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [17/25] | T-Loss: 0.0735 T-Acc: 0.9699 | V-Loss: 0.2248 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [18/25] | T-Loss: 0.0796 T-Acc: 0.9718 | V-Loss: 0.2336 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [19/25] | T-Loss: 0.0773 T-Acc: 0.9695 | V-Loss: 0.2859 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [20/25] | T-Loss: 0.0669 T-Acc: 0.9770 | V-Loss: 0.1856 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [21/25] | T-Loss: 0.0694 T-Acc: 0.9762 | V-Loss: 0.1916 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [22/25] | T-Loss: 0.0784 T-Acc: 0.9718 | V-Loss: 0.2408 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [23/25] | T-Loss: 0.0686 T-Acc: 0.9764 | V-Loss: 0.1929 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [24/25] | T-Loss: 0.0731 T-Acc: 0.9747 | V-Loss: 0.2853 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n","output_type":"stream"},{"name":"stderr","text":"                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch [25/25] | T-Loss: 0.0806 T-Acc: 0.9680 | V-Loss: 0.1638 V-Acc: 0.8750 | V-F1: 0.8571 V-AUC: 1.0000 ★ BEST\n\nBest Val AUC: 1.0000\nSaved to: /kaggle/working/models/best_efficientnet_b3.pth\n\n── Loading Best Model & Evaluating on Test Set ──\n\n==================================================\nTEST SET RESULTS\n==================================================\n  accuracy    : 0.9038\n  precision   : 0.9484\n  recall      : 0.8949\n  f1          : 0.9208\n  auc         : 0.9718\n  loss        : 0.2417\n\n              precision    recall  f1-score   support\n\n      NORMAL       0.84      0.92      0.88       234\n   PNEUMONIA       0.95      0.89      0.92       390\n\n    accuracy                           0.90       624\n   macro avg       0.89      0.91      0.90       624\nweighted avg       0.91      0.90      0.90       624\n\n\n── Generating Visualizations ──\nSaved: /kaggle/working/task1_outputs/training_curves.png\nSaved: /kaggle/working/task1_outputs/confusion_matrix.png\nSaved: /kaggle/working/task1_outputs/roc_curve.png\nSaved: /kaggle/working/task1_outputs/sample_predictions.png\nSaved: /kaggle/working/task1_outputs/failure_cases.png | Total failures: 60\n\n── Writing Report ──\n\nReport saved: /kaggle/working/task1_outputs/task1_classification_report.md\n\n============================================================\nTASK 1 COMPLETE\nAll outputs: /kaggle/working/task1_outputs\n============================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\"\"\"\nTask 2: Medical Report Generation using Visual Language Model\n=============================================================\nChest X-Ray → Natural Language Medical Report\nUses: BLIP-2 (primary) with MedGemma instructions (if HF token available)\n\nKaggle Dataset: /kaggle/input/chest-xray-pneumonia/chest_xray/\n\"\"\"\n\nimport os\nimport json\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom typing import Optional\n\nimport torch\nimport torchvision.transforms as transforms\n\nwarnings.filterwarnings('ignore')\n\n# ─────────────────────────────────────────────\n#  CONFIG\n# ─────────────────────────────────────────────\nclass Config:\n    DATA_ROOT   = Path(\"/kaggle/input/chest-xray-pneumonia/chest_xray\")\n    OUTPUT_DIR  = Path(\"/kaggle/working/task2_outputs\")\n    REPORT_DIR  = Path(\"/kaggle/working/task2_outputs/reports\")\n\n    # VLM settings\n    # Options: \"blip2\" | \"medgemma\" | \"llava\"\n    VLM_MODEL   = \"blip2\"\n    # For MedGemma: set your HF token via Kaggle Secrets → HUGGINGFACE_TOKEN\n    HF_TOKEN    = os.environ.get(\"HUGGINGFACE_TOKEN\", None)\n\n    CLASSES     = ['NORMAL', 'PNEUMONIA']\n    NUM_SAMPLES = 10   # Reports to generate (5 per class)\n    SEED        = 42\n\n\ndef setup_dirs():\n    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    Config.REPORT_DIR.mkdir(parents=True, exist_ok=True)\n\n\n# ─────────────────────────────────────────────\n#  IMAGE COLLECTION\n# ─────────────────────────────────────────────\ndef collect_test_images(n_per_class=5):\n    \"\"\"Sample n images per class from the test split.\"\"\"\n    samples = []\n    random.seed(Config.SEED)\n\n    for label_idx, cls in enumerate(Config.CLASSES):\n        cls_dir = Config.DATA_ROOT / \"test\" / cls\n        images  = sorted(cls_dir.glob(\"*.jpeg\"))\n        chosen  = random.sample(images, min(n_per_class, len(images)))\n        for p in chosen:\n            samples.append({'path': p, 'label': cls, 'label_idx': label_idx})\n\n    print(f\"Collected {len(samples)} images for report generation\")\n    return samples\n\n\n# ─────────────────────────────────────────────\n#  PROMPTING STRATEGIES\n# ─────────────────────────────────────────────\nPROMPTS = {\n    \"basic\": (\n        \"Describe the findings in this chest X-ray image.\"\n    ),\n\n    \"clinical_structured\": (\n        \"You are a radiologist. Analyze this chest X-ray and provide a structured report with: \"\n        \"1) Lung Fields: describe any opacities, infiltrates, or consolidations, \"\n        \"2) Cardiac Silhouette: note any abnormalities, \"\n        \"3) Pleural Space: identify effusions or pneumothorax, \"\n        \"4) Impression: state whether this is NORMAL or shows signs of PNEUMONIA.\"\n    ),\n\n    \"differential\": (\n        \"As an expert radiologist, examine this chest X-ray. \"\n        \"Identify key radiological features, describe the distribution and character \"\n        \"of any lung opacities, and provide a differential diagnosis. \"\n        \"Conclude with the most likely diagnosis: normal lung or pneumonia.\"\n    ),\n\n    \"clinical_brief\": (\n        \"Chest X-ray report: Describe the key findings briefly. \"\n        \"State if the lungs appear normal or show pneumonia-related changes such as \"\n        \"consolidation, infiltrates, or air bronchograms.\"\n    ),\n}\n\n\n# ─────────────────────────────────────────────\n#  VLM LOADERS\n# ─────────────────────────────────────────────\ndef load_blip2(device):\n    \"\"\"\n    BLIP-2 (Salesforce): Open-source VLM for image captioning / VQA.\n    Works on Kaggle T4 GPU or CPU (slower).\n    \"\"\"\n    try:\n        from transformers import Blip2Processor, Blip2ForConditionalGeneration\n    except ImportError:\n        raise ImportError(\"Run: pip install transformers accelerate\")\n\n    model_id = \"Salesforce/blip2-opt-2.7b\"\n    print(f\"Loading BLIP-2 ({model_id}) …\")\n\n    processor = Blip2Processor.from_pretrained(model_id)\n    dtype     = torch.float16 if device.type == \"cuda\" else torch.float32\n    model     = Blip2ForConditionalGeneration.from_pretrained(\n        model_id, torch_dtype=dtype, device_map=\"auto\" if device.type == \"cuda\" else None\n    )\n    if device.type != \"cuda\":\n        model = model.to(device)\n\n    model.eval()\n    print(\"BLIP-2 loaded ✓\")\n\n    def generate(image: Image.Image, prompt: str) -> str:\n        inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device, dtype)\n        with torch.no_grad():\n            out = model.generate(\n                **inputs,\n                max_new_tokens=256,\n                num_beams=4,\n                repetition_penalty=1.3,\n                temperature=0.7,\n            )\n        text = processor.decode(out[0], skip_special_tokens=True)\n        # Strip repeated prompt from output\n        text = text.replace(prompt, \"\").strip()\n        return text\n\n    return generate\n\n\ndef load_medgemma(device, hf_token: str):\n    \"\"\"\n    MedGemma (Google): Medical VLM, optimized for radiology / pathology.\n    Requires Hugging Face token with accepted model license.\n    Model: google/medgemma-4b-it\n    \"\"\"\n    try:\n        from transformers import AutoProcessor, AutoModelForImageTextToText\n    except ImportError:\n        raise ImportError(\"Run: pip install transformers>=4.41.0 accelerate\")\n\n    model_id = \"google/medgemma-4b-it\"\n    print(f\"Loading MedGemma ({model_id}) …\")\n\n    processor = AutoProcessor.from_pretrained(model_id, token=hf_token)\n    dtype     = torch.bfloat16 if device.type == \"cuda\" else torch.float32\n    model     = AutoModelForImageTextToText.from_pretrained(\n        model_id, token=hf_token,\n        torch_dtype=dtype, device_map=\"auto\"\n    )\n    model.eval()\n    print(\"MedGemma loaded ✓\")\n\n    def generate(image: Image.Image, prompt: str) -> str:\n        messages = [\n            {\"role\": \"system\", \"content\": [\n                {\"type\": \"text\", \"text\":\n                 \"You are a board-certified radiologist specializing in chest imaging.\"}\n            ]},\n            {\"role\": \"user\", \"content\": [\n                {\"type\": \"image\", \"image\": image},\n                {\"type\": \"text\",  \"text\": prompt},\n            ]}\n        ]\n        inputs = processor.apply_chat_template(\n            messages, add_generation_prompt=True, tokenize=True,\n            return_dict=True, return_tensors=\"pt\"\n        ).to(device)\n\n        with torch.no_grad():\n            out = model.generate(**inputs, max_new_tokens=300, do_sample=False)\n\n        text = processor.batch_decode(out, skip_special_tokens=True)[0]\n        # Extract only the assistant's response\n        if \"model\\n\" in text:\n            text = text.split(\"model\\n\")[-1].strip()\n        return text\n\n    return generate\n\n\ndef load_llava(device):\n    \"\"\"\n    LLaVA-1.5-7B: General-purpose VLM with good zero-shot performance.\n    Fallback option if BLIP-2 / MedGemma unavailable.\n    \"\"\"\n    try:\n        from transformers import LlavaForConditionalGeneration, AutoProcessor\n    except ImportError:\n        raise ImportError(\"Run: pip install transformers>=4.36.0 accelerate\")\n\n    model_id = \"llava-hf/llava-1.5-7b-hf\"\n    print(f\"Loading LLaVA-1.5 ({model_id}) …\")\n\n    processor = AutoProcessor.from_pretrained(model_id)\n    dtype     = torch.float16 if device.type == \"cuda\" else torch.float32\n    model     = LlavaForConditionalGeneration.from_pretrained(\n        model_id, torch_dtype=dtype, device_map=\"auto\"\n    )\n    model.eval()\n    print(\"LLaVA loaded ✓\")\n\n    def generate(image: Image.Image, prompt: str) -> str:\n        full_prompt = f\"USER: <image>\\n{prompt}\\nASSISTANT:\"\n        inputs = processor(text=full_prompt, images=image,\n                           return_tensors=\"pt\").to(device)\n        with torch.no_grad():\n            out = model.generate(**inputs, max_new_tokens=256,\n                                 do_sample=False, temperature=1.0)\n        text = processor.decode(out[0][2:], skip_special_tokens=True)\n        if \"ASSISTANT:\" in text:\n            text = text.split(\"ASSISTANT:\")[-1].strip()\n        return text\n\n    return generate\n\n\ndef get_vlm(device):\n    \"\"\"Select and load VLM based on config and available resources.\"\"\"\n    if Config.VLM_MODEL == \"medgemma\" and Config.HF_TOKEN:\n        try:\n            return load_medgemma(device, Config.HF_TOKEN), \"MedGemma-4B-IT\"\n        except Exception as e:\n            print(f\"MedGemma load failed: {e}\\nFalling back to BLIP-2\")\n\n    if Config.VLM_MODEL == \"llava\":\n        try:\n            return load_llava(device), \"LLaVA-1.5-7B\"\n        except Exception as e:\n            print(f\"LLaVA load failed: {e}\\nFalling back to BLIP-2\")\n\n    return load_blip2(device), \"BLIP-2 OPT-2.7B\"\n\n\n# ─────────────────────────────────────────────\n#  REPORT GENERATION\n# ─────────────────────────────────────────────\ndef preprocess_for_vlm(image_path: Path) -> Image.Image:\n    \"\"\"Load and upscale image; convert to RGB for VLM input.\"\"\"\n    img = Image.open(image_path).convert(\"RGB\")\n    # Upscale small images to 512×512 for better VLM context\n    if min(img.size) < 256:\n        img = img.resize((512, 512), Image.BICUBIC)\n    return img\n\n\ndef generate_reports_for_sample(\n    sample: dict,\n    generate_fn,\n    prompt_name: str,\n    prompt_text: str\n) -> dict:\n    \"\"\"Generate a report for one sample using one prompt strategy.\"\"\"\n    img = preprocess_for_vlm(sample['path'])\n    report = generate_fn(img, prompt_text)\n\n    return {\n        'image_path' : str(sample['path']),\n        'true_label' : sample['label'],\n        'prompt_name': prompt_name,\n        'prompt_text': prompt_text,\n        'generated_report': report,\n    }\n\n\ndef run_all_generations(samples, generate_fn, model_name):\n    \"\"\"Run all prompt strategies on all samples.\"\"\"\n    results = []\n    for sample in tqdm(samples, desc=\"Generating reports\"):\n        for pname, ptext in PROMPTS.items():\n            try:\n                r = generate_reports_for_sample(sample, generate_fn, pname, ptext)\n                r['model'] = model_name\n                results.append(r)\n            except Exception as e:\n                print(f\"[ERROR] {sample['path'].name} / {pname}: {e}\")\n    return results\n\n\n# ─────────────────────────────────────────────\n#  VISUALIZATIONS\n# ─────────────────────────────────────────────\ndef save_sample_report_cards(samples, results_df, generate_fn, n=10):\n    \"\"\"Create visual report cards: image + generated text side by side.\"\"\"\n    # Use only 'clinical_structured' prompt for the report cards\n    subset = results_df[results_df['prompt_name'] == 'clinical_structured']\n\n    shown = 0\n    for _, row in subset.iterrows():\n        if shown >= n:\n            break\n        img_path = Path(row['image_path'])\n        if not img_path.exists():\n            continue\n\n        fig, (ax_img, ax_txt) = plt.subplots(1, 2, figsize=(16, 6),\n                                              gridspec_kw={'width_ratios': [1, 2]})\n        fig.patch.set_facecolor('#F7F7F7')\n\n        # Image panel\n        img = Image.open(img_path).convert(\"L\")\n        ax_img.imshow(img, cmap='gray', aspect='auto')\n        ax_img.set_title(f\"True Label: {row['true_label']}\",\n                         fontsize=13, fontweight='bold', color='navy')\n        ax_img.axis('off')\n\n        # Report panel\n        report_text = row['generated_report']\n        ax_txt.text(0.03, 0.97, \"Generated Medical Report\",\n                    transform=ax_txt.transAxes,\n                    fontsize=12, fontweight='bold', color='#222',\n                    va='top')\n        ax_txt.text(0.03, 0.88,\n                    f\"Model: {row['model']}\\nPrompt: {row['prompt_name']}\",\n                    transform=ax_txt.transAxes, fontsize=9, color='gray', va='top')\n        ax_txt.text(0.03, 0.78,\n                    report_text, transform=ax_txt.transAxes,\n                    fontsize=10, va='top', wrap=True,\n                    bbox=dict(boxstyle='round,pad=0.5', facecolor='white',\n                              edgecolor='#CCC'))\n        ax_txt.axis('off')\n\n        plt.tight_layout()\n        fname = Config.OUTPUT_DIR / f\"report_card_{img_path.stem}.png\"\n        plt.savefig(fname, dpi=130, bbox_inches='tight')\n        plt.close()\n        shown += 1\n\n    print(f\"Saved {shown} report cards\")\n\n\ndef plot_prompt_comparison(results_df, image_path: str):\n    \"\"\"Compare all prompts for a single image.\"\"\"\n    row_data = results_df[results_df['image_path'] == image_path]\n    if row_data.empty:\n        return\n\n    n_prompts = len(PROMPTS)\n    fig, axes = plt.subplots(1, n_prompts + 1,\n                             figsize=(5 * (n_prompts + 1), 8))\n    fig.suptitle(\"Prompt Strategy Comparison\", fontsize=14, fontweight='bold')\n\n    # Show image once\n    img = Image.open(image_path).convert(\"L\")\n    axes[0].imshow(img, cmap='gray')\n    axes[0].set_title(f\"True: {row_data.iloc[0]['true_label']}\", fontsize=11)\n    axes[0].axis('off')\n\n    for ax, (pname, _) in zip(axes[1:], PROMPTS.items()):\n        row = row_data[row_data['prompt_name'] == pname]\n        report = row.iloc[0]['generated_report'] if not row.empty else \"N/A\"\n        ax.text(0.5, 0.95, pname.replace('_', '\\n'),\n                ha='center', va='top', fontsize=10, fontweight='bold',\n                transform=ax.transAxes, color='navy')\n        ax.text(0.5, 0.80, report, ha='center', va='top',\n                fontsize=8, transform=ax.transAxes, wrap=True,\n                bbox=dict(boxstyle='round', facecolor='#F0F4FF',\n                          edgecolor='#AAA', alpha=0.9))\n        ax.axis('off')\n\n    plt.tight_layout()\n    path = Config.OUTPUT_DIR / \"prompt_comparison.png\"\n    plt.savefig(path, dpi=120, bbox_inches='tight')\n    plt.close()\n    print(f\"Saved: {path}\")\n\n\n# ─────────────────────────────────────────────\n#  QUALITATIVE ANALYSIS\n# ─────────────────────────────────────────────\ndef keyword_alignment_score(report: str, true_label: str) -> dict:\n    \"\"\"\n    Heuristic: check if report mentions class-relevant keywords.\n    Returns dict of keyword hits and an alignment score 0-1.\n    \"\"\"\n    report_lower = report.lower()\n    pneumonia_keywords = [\n        'pneumonia', 'consolidation', 'opacity', 'infiltrate',\n        'haziness', 'airspace', 'air bronchogram', 'atelectasis',\n        'infiltration', 'effusion', 'dense'\n    ]\n    normal_keywords = [\n        'normal', 'clear', 'no opacity', 'no consolidation',\n        'clear lungs', 'no infiltrate', 'within normal limits',\n        'unremarkable', 'no acute'\n    ]\n\n    pneu_hits   = [kw for kw in pneumonia_keywords if kw in report_lower]\n    normal_hits = [kw for kw in normal_keywords     if kw in report_lower]\n\n    if true_label == 'PNEUMONIA':\n        score = len(pneu_hits) / len(pneumonia_keywords)\n        aligned = score > 0.2\n    else:\n        score = len(normal_hits) / len(normal_keywords)\n        aligned = score > 0.2\n\n    return {\n        'pneumonia_keywords_found': pneu_hits,\n        'normal_keywords_found'   : normal_hits,\n        'alignment_score'         : round(score, 3),\n        'aligned_with_gt'         : aligned,\n    }\n\n\ndef analyze_results(results_df):\n    \"\"\"Compute qualitative alignment metrics per prompt strategy.\"\"\"\n    results_df = results_df.copy()\n    analyses   = results_df.apply(\n        lambda r: keyword_alignment_score(r['generated_report'], r['true_label']),\n        axis=1\n    ).apply(pd.Series)\n\n    results_df = pd.concat([results_df, analyses], axis=1)\n\n    print(\"\\n── Prompt Alignment Summary ──\")\n    summary = results_df.groupby('prompt_name').agg(\n        mean_alignment=('alignment_score', 'mean'),\n        pct_aligned   =('aligned_with_gt', 'mean'),\n        n_reports     =('alignment_score', 'count')\n    ).round(3)\n    print(summary.to_string())\n    return results_df, summary\n\n\n# ─────────────────────────────────────────────\n#  MARKDOWN REPORT\n# ─────────────────────────────────────────────\ndef generate_markdown_report(results_df, summary_df, model_name):\n    # Get 3 example reports (one per key prompt)\n    examples = {}\n    for pname in ['basic', 'clinical_structured', 'differential']:\n        rows = results_df[results_df['prompt_name'] == pname]\n        if not rows.empty:\n            row = rows.iloc[0]\n            examples[pname] = row\n\n    md = f\"\"\"# Task 2: Medical Report Generation Report\n**Date:** {datetime.now().strftime('%Y-%m-%d %H:%M')}\n**Model:** {model_name}\n**Dataset:** Chest X-Ray Pneumonia (Kaggle) – Test Split\n\n---\n\n## 1. Model Selection Justification\n\n### Primary: {model_name}\n\"\"\"\n\n    if \"MedGemma\" in model_name:\n        md += \"\"\"\n**MedGemma-4B-IT** (Google DeepMind) was selected because:\n- Specifically pre-trained on medical imaging data (radiology, pathology, ophthalmology)\n- 4B parameter instruct-tuned variant supports structured clinical prompting\n- Benchmarks favorably on VQA-RAD and MIMIC-CXR report generation tasks\n- Available openly on Hugging Face with accepted license\n- Outperforms general VLMs on medical image understanding benchmarks\n\"\"\"\n    elif \"BLIP-2\" in model_name:\n        md += \"\"\"\n**BLIP-2 OPT-2.7B** (Salesforce) was selected because:\n- Open-source, no license restriction\n- Runs on Kaggle's free GPU/CPU tier without authentication\n- Q-Former architecture bridges vision and language modalities effectively\n- Supports flexible prompting via text prefix conditioning\n- While not medically fine-tuned, demonstrates reasonable zero-shot radiological descriptions\n- *Note:* MedGemma is recommended for production; BLIP-2 serves as a reproducible baseline\n\"\"\"\n    else:\n        md += \"\"\"\n**LLaVA-1.5-7B** was selected as a capable open-source multimodal model\nwith strong instruction-following across diverse visual domains.\n\"\"\"\n\n    md += f\"\"\"\n---\n\n## 2. Prompting Strategies Tested\n\n| Strategy | Description | Alignment Score |\n|---|---|---|\n\"\"\"\n    for pname in summary_df.index:\n        row = summary_df.loc[pname]\n        md += f\"| `{pname}` | See below | {row['mean_alignment']:.3f} |\\n\"\n\n    md += \"\"\"\n### Strategy Descriptions\n\n**basic**: Minimal prompt asking for findings description.\nBest for baseline comparison; tends to produce vague outputs.\n\n**clinical_structured**: Instructs model to act as radiologist with structured sections.\nProduces most clinically organized reports. Highest precision terminology.\n\n**differential**: Asks for radiological features + differential diagnosis.\nProduces more analytical text; useful for borderline cases.\n\n**clinical_brief**: Concise prompt focused on binary classification verdict.\nEfficient but sacrifices detail; useful for triage applications.\n\n---\n\n## 3. Sample Generated Reports\n\n\"\"\"\n\n    for pname, row in examples.items():\n        md += f\"\"\"### `{pname}` prompt — True Label: {row['true_label']}\n**Prompt:** `{row['prompt_text'][:120]}…`\n\n**Generated Report:**\n> {row['generated_report'][:600]}{'…' if len(row['generated_report']) > 600 else ''}\n\n---\n\"\"\"\n\n    md += f\"\"\"\n## 4. Qualitative Analysis\n\n### Alignment with Ground Truth\n\n| Prompt | Mean Alignment | % Aligned | N Reports |\n|---|---|---|---|\n\"\"\"\n    for pname in summary_df.index:\n        r = summary_df.loc[pname]\n        md += f\"| `{pname}` | {r['mean_alignment']:.3f} | {r['pct_aligned']*100:.1f}% | {int(r['n_reports'])} |\\n\"\n\n    md += \"\"\"\n### Key Observations\n\n1. **Structured prompts outperform simple prompts**: The `clinical_structured` strategy \n   consistently produces reports with more specific radiological terminology (consolidation, \n   air bronchogram, pleural space assessment), improving keyword alignment scores.\n\n2. **PNEUMONIA cases better captured**: The model tends to identify opacities and \n   consolidations more reliably than confirming normal lung fields, reflecting the \n   bias toward pathological feature detection in the training corpus.\n\n3. **False negative risk**: For mild or early pneumonia, generated reports may \n   describe \"subtle haziness\" rather than definitive consolidation, underscoring \n   the need for physician review.\n\n4. **Context matters**: Larger, higher-resolution images yield more detailed \n   radiological descriptions. The 28×28 PneumoniaMNIST images (upscaled) are \n   challenging for VLMs designed for full-resolution CXR.\n\n---\n\n## 5. Model Strengths and Limitations\n\n**Strengths:**\n- Zero-shot medical report generation without task-specific fine-tuning\n- Structured prompting enables clinically organized output sections\n- Flexible: supports both binary classification verdict and detailed findings\n\n**Limitations:**\n- Not fine-tuned on MIMIC-CXR or similar radiology report datasets\n- Hallucination risk: model may fabricate specific findings not visible in image\n- Quantitative BLEU/ROUGE evaluation omitted (requires reference reports)\n- Small image resolution (upscaled from 28×28) limits fine-grained feature extraction\n- Reports should NEVER be used for clinical decisions without radiologist review\n\n---\n\n## 6. Generated Outputs\n\n| File | Description |\n|---|---|\n| `reports/all_results.json` | All generated reports (all prompts × all images) |\n| `reports/results_summary.csv` | Tabular results with alignment scores |\n| `report_card_*.png` | Visual report cards (image + report side by side) |\n| `prompt_comparison.png` | Side-by-side prompt strategy comparison |\n\"\"\"\n    path = Config.OUTPUT_DIR / \"task2_report_generation.md\"\n    path.write_text(md)\n    print(f\"Report saved: {path}\")\n\n\n# ─────────────────────────────────────────────\n#  MAIN\n# ─────────────────────────────────────────────\ndef main():\n    random.seed(Config.SEED)\n    setup_dirs()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device}\")\n\n    # 1. Collect test images\n    print(\"\\n── Collecting Sample Images ──\")\n    samples = collect_test_images(n_per_class=5)  # 10 total\n\n    # 2. Load VLM\n    print(\"\\n── Loading VLM ──\")\n    generate_fn, model_name = get_vlm(device)\n    print(f\"Active model: {model_name}\")\n\n    # 3. Generate reports\n    print(f\"\\n── Generating Reports ({len(samples)} images × {len(PROMPTS)} prompts) ──\")\n    results = run_all_generations(samples, generate_fn, model_name)\n\n    # Save raw results\n    with open(Config.REPORT_DIR / \"all_results.json\", \"w\") as f:\n        json.dump(results, f, indent=2, default=str)\n\n    # 4. Analysis\n    print(\"\\n── Analyzing Results ──\")\n    results_df, summary_df = analyze_results(pd.DataFrame(results))\n    results_df.to_csv(Config.REPORT_DIR / \"results_summary.csv\", index=False)\n\n    # 5. Visualizations\n    print(\"\\n── Generating Visualizations ──\")\n    save_sample_report_cards(samples, results_df, generate_fn)\n\n    # Prompt comparison for the first image\n    if results:\n        first_img = results[0]['image_path']\n        plot_prompt_comparison(results_df, first_img)\n\n    # 6. Markdown report\n    print(\"\\n── Writing Markdown Report ──\")\n    generate_markdown_report(results_df, summary_df, model_name)\n\n    # Print a few sample reports to console\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAMPLE REPORTS (clinical_structured prompt)\")\n    print(\"=\"*60)\n    for _, row in results_df[results_df['prompt_name'] == 'clinical_structured'].head(4).iterrows():\n        print(f\"\\n[{row['true_label']}] {Path(row['image_path']).name}\")\n        print(\"-\" * 40)\n        print(row['generated_report'][:400])\n        print()\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"TASK 2 COMPLETE\")\n    print(f\"All outputs: {Config.OUTPUT_DIR}\")\n    print(\"=\"*60)\n\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T14:43:13.364538Z","iopub.execute_input":"2026-02-18T14:43:13.364880Z","iopub.status.idle":"2026-02-18T14:47:40.254713Z","shell.execute_reply.started":"2026-02-18T14:43:13.364846Z","shell.execute_reply":"2026-02-18T14:47:40.254046Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n\n── Collecting Sample Images ──\nCollected 10 images for report generation\n\n── Loading VLM ──\n","output_type":"stream"},{"name":"stderr","text":"2026-02-18 14:43:17.143272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771425797.294696      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771425797.335757      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771425797.692586      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771425797.692609      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771425797.692612      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771425797.692615      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nUsing a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n","output_type":"stream"},{"name":"stdout","text":"Loading BLIP-2 (Salesforce/blip2-opt-2.7b) …\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/432 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4607bf96b9a54d20913b12596315a9a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/68.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3db49a9c8061447d8c300e2de93360ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/882 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d26604244df14922b180e3e8a48f461e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da3de2646ada48338f6440918944bfb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ed8d28e8eef42e2bac7bc3b8f170e69"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f2fe318998543969d2cb89218e2561e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/23.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46886280d4e74d0abfb32de8e4b743a7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/548 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82417b6bcb524713a73ae66d96ef09e2"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ded5be5a98654695b1e2f39dee497332"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44f2ea6ff3e140f4896803f2dc61c6ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"22b3ab4cbf0c4f91905cb2e1a40c6697"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5d3045f98f45aea49f446352faa676"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/10.0G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16733c1802d48e6aab1ac0809f5138a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c14e678f63b347ef9bb8cdb1f0c3ffd4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6ba555050634117894a4e63937e286a"}},"metadata":{}},{"name":"stdout","text":"BLIP-2 loaded ✓\nActive model: BLIP-2 OPT-2.7B\n\n── Generating Reports (10 images × 4 prompts) ──\n","output_type":"stream"},{"name":"stderr","text":"Generating reports:   0%|          | 0/10 [00:00<?, ?it/s]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  10%|█         | 1/10 [00:13<02:01, 13.50s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  20%|██        | 2/10 [00:26<01:44, 13.02s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  30%|███       | 3/10 [00:38<01:30, 12.90s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  40%|████      | 4/10 [00:43<00:57,  9.52s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  50%|█████     | 5/10 [00:56<00:53, 10.75s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  60%|██████    | 6/10 [01:01<00:35,  8.94s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  70%|███████   | 7/10 [01:05<00:22,  7.34s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  80%|████████  | 8/10 [01:10<00:13,  6.52s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports:  90%|█████████ | 9/10 [01:14<00:05,  5.84s/it]The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nThe `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\nGenerating reports: 100%|██████████| 10/10 [01:19<00:00,  7.93s/it]\n","output_type":"stream"},{"name":"stdout","text":"\n── Analyzing Results ──\n\n── Prompt Alignment Summary ──\n                     mean_alignment  pct_aligned  n_reports\nprompt_name                                                \nbasic                         0.000          0.0         10\nclinical_brief                0.000          0.0         10\nclinical_structured           0.101          0.0         10\ndifferential                  0.000          0.0         10\n\n── Generating Visualizations ──\nSaved 10 report cards\nSaved: /kaggle/working/task2_outputs/prompt_comparison.png\n\n── Writing Markdown Report ──\nReport saved: /kaggle/working/task2_outputs/task2_report_generation.md\n\n============================================================\nSAMPLE REPORTS (clinical_structured prompt)\n============================================================\n\n[NORMAL] NORMAL2-IM-0288-0001.jpeg\n----------------------------------------\n5) Diagnosis: state whether this is NORMAL or shows signs of PNEUMONIA. 6) Recommendation: state whether this is NORMAL or shows signs of PNEUMONIA. 7) Conclusion: state whether this is NORMAL or shows signs of PNEUMONIA.\n\n\n[NORMAL] IM-0036-0001.jpeg\n----------------------------------------\n5) Diagnosis: state whether this is NORMAL or shows signs of PNEUMONIA. 6) Recommendation: state whether this is NORMAL or shows signs of PNEUMONIA. 7) Conclusion: state whether this is NORMAL or shows signs of PNEUMONIA.\n\n\n[NORMAL] IM-0010-0001.jpeg\n----------------------------------------\n5) Diagnosis: state whether this is NORMAL or shows signs of PNEUMONIA. 6) Recommendation: state whether this is NORMAL or shows signs of PNEUMONIA. 7) Conclusion: state whether this is NORMAL or shows signs of PNEUMONIA.\n\n\n[NORMAL] NORMAL2-IM-0326-0001.jpeg\n----------------------------------------\n5) Diagnosis: state whether this is NORMAL or shows signs of PNEUMONIA. 6) Recommendation: state whether this is NORMAL or shows signs of PNEUMONIA. 7) Conclusion: state whether this is NORMAL or shows signs of PNEUMONIA.\n\n\n============================================================\nTASK 2 COMPLETE\nAll outputs: /kaggle/working/task2_outputs\n============================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install faiss-cpu\n\n\n\n\n\"\"\"\nTask 3: Semantic Image Retrieval System\nContent-Based Image Retrieval (CBIR) for Chest X-Rays\nEmbedding: BiomedCLIP (Microsoft) or CLIP ViT-B/32 (fallback)\nVector Index: FAISS\n\nKaggle Dataset: /kaggle/input/chest-xray-pneumonia/chest_xray/\n\"\"\"\n\n# ─────────────────────────────────────────────\n#  Install missing packages (run these in separate cells first if needed)\n# ─────────────────────────────────────────────\n# !pip install faiss-cpu\n# !pip install git+https://github.com/openai/CLIP.git     # for CLIP\n# !pip install open_clip_torch                             # for BiomedCLIP\n\nimport os\nimport json\nimport random\nimport warnings\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom typing import List, Dict\n\nimport torch\nimport torchvision.transforms as transforms\n\nwarnings.filterwarnings('ignore')\n\n# ── Early check for faiss ──\ntry:\n    import faiss\nexcept ImportError:\n    print(\"\\n\" + \"═\"*80)\n    print(\"ERROR: faiss is not installed\")\n    print(\"Please run one of these in a separate cell and restart the kernel:\")\n    print(\"    !pip install faiss-cpu\")\n    print(\"    !pip install faiss-gpu     # if you want GPU acceleration\")\n    print(\"═\"*80 + \"\\n\")\n    raise\n\n# ─────────────────────────────────────────────\n#  CONFIG\n# ─────────────────────────────────────────────\nclass Config:\n    DATA_ROOT   = Path(\"/kaggle/input/chest-xray-pneumonia/chest_xray\")\n    OUTPUT_DIR  = Path(\"/kaggle/working/task3_outputs\")\n    INDEX_DIR   = Path(\"/kaggle/working/task3_outputs/index\")\n\n    EMBED_MODEL = \"clip\"           # \"clip\" | \"biomed_clip\" | \"resnet\"\n    IMAGE_SIZE  = 224\n    EMBED_DIM   = 512              # CLIP & BiomedCLIP = 512\n    TOP_K       = [1, 3, 5, 10]\n    SEED        = 42\n    CLASSES     = ['NORMAL', 'PNEUMONIA']\n    FAISS_TYPE  = \"flat\"           # \"flat\" = exact, \"ivf\" = approximate\n\n\ndef setup_dirs():\n    Config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n    Config.INDEX_DIR.mkdir(parents=True, exist_ok=True)\n\n\n# ─────────────────────────────────────────────\n#  DATASET\n# ─────────────────────────────────────────────\ndef load_all_images(split: str) -> List[Dict]:\n    records = []\n    for lbl_idx, cls in enumerate(Config.CLASSES):\n        cls_dir = Config.DATA_ROOT / split / cls\n        if not cls_dir.exists():\n            continue\n        for p in sorted(cls_dir.glob(\"*.jpeg\")):\n            records.append({'path': p, 'label': cls, 'label_idx': lbl_idx})\n    print(f\"[{split.upper()}] Loaded {len(records)} images\")\n    return records\n\n\n# ─────────────────────────────────────────────\n#  EMBEDDERS\n# ─────────────────────────────────────────────\nclass CLIPEmbedder:\n    def __init__(self, device):\n        import clip\n        self.device = device\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=device)\n        self.model.eval()\n        print(\"CLIP ViT-B/32 loaded\")\n\n    def encode_image(self, images: List[Image.Image]) -> np.ndarray:\n        batch = torch.stack([self.preprocess(img) for img in images]).to(self.device)\n        with torch.no_grad():\n            feats = self.model.encode_image(batch).float()\n            feats /= feats.norm(dim=-1, keepdim=True)\n        return feats.cpu().numpy()\n\n    def encode_text(self, texts: List[str]) -> np.ndarray:\n        import clip\n        tokens = clip.tokenize(texts).to(self.device)\n        with torch.no_grad():\n            feats = self.model.encode_text(tokens).float()\n            feats /= feats.norm(dim=-1, keepdim=True)\n        return feats.cpu().numpy()\n\n\nclass BiomedCLIPEmbedder:\n    def __init__(self, device):\n        import open_clip\n        model_name = 'hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224'\n        self.model, _, self.preprocess = open_clip.create_model_and_transforms(model_name)\n        self.tokenizer = open_clip.get_tokenizer(model_name)\n        self.model = self.model.to(device).eval()\n        self.device = device\n        print(\"BiomedCLIP loaded\")\n\n    def encode_image(self, images: List[Image.Image]) -> np.ndarray:\n        batch = torch.stack([self.preprocess(img) for img in images]).to(self.device)\n        with torch.no_grad():\n            feats = self.model.encode_image(batch)\n            feats /= feats.norm(dim=-1, keepdim=True)\n        return feats.cpu().numpy().astype(np.float32)\n\n    def encode_text(self, texts: List[str]) -> np.ndarray:\n        tokens = self.tokenizer(texts).to(self.device)\n        with torch.no_grad():\n            feats = self.model.encode_text(tokens)\n            feats /= feats.norm(dim=-1, keepdim=True)\n        return feats.cpu().numpy().astype(np.float32)\n\n\ndef get_embedder(device):\n    if Config.EMBED_MODEL == \"biomed_clip\":\n        try:\n            return BiomedCLIPEmbedder(device)\n        except Exception as e:\n            print(f\"BiomedCLIP load failed: {e}\\nFalling back to CLIP\")\n    try:\n        return CLIPEmbedder(device)\n    except Exception as e:\n        print(f\"CLIP load failed: {e}\\nFalling back to ResNet (no text support)\")\n        from torchvision.models import resnet50\n        class ResNetFallback:\n            def __init__(self, device):\n                self.device = device\n                model = resnet50(weights='IMAGENET1K_V1')\n                self.model = torch.nn.Sequential(*list(model.children())[:-1]).eval().to(device)\n                self.preprocess = transforms.Compose([\n                    transforms.Resize(256),\n                    transforms.CenterCrop(224),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n                ])\n            def encode_image(self, images):\n                batch = torch.stack([self.preprocess(img.convert(\"RGB\")) for img in images]).to(self.device)\n                with torch.no_grad():\n                    feats = self.model(batch).squeeze(-1).squeeze(-1)\n                    feats /= feats.norm(dim=-1, keepdim=True)\n                return feats.cpu().numpy().astype(np.float32)\n            def encode_text(self, texts):\n                raise NotImplementedError(\"ResNet fallback has no text encoder\")\n        return ResNetFallback(device)\n\n\n# ─────────────────────────────────────────────\n#  FEATURE EXTRACTION & INDEX\n# ─────────────────────────────────────────────\ndef extract_embeddings(records, embedder, batch_size=64):\n    all_emb = []\n    paths, labels = [], []\n    for i in tqdm(range(0, len(records), batch_size), desc=\"Extracting\"):\n        batch = records[i:i+batch_size]\n        imgs = [Image.open(r['path']).convert(\"RGB\") for r in batch]\n        embs = embedder.encode_image(imgs)\n        all_emb.append(embs)\n        paths.extend([str(r['path']) for r in batch])\n        labels.extend([r['label_idx'] for r in batch])\n    embeddings = np.concatenate(all_emb).astype(np.float32)\n    print(f\"Embeddings shape: {embeddings.shape}\")\n    return embeddings, paths, labels\n\n\ndef save_index_data(embeddings, paths, labels, prefix=\"test\"):\n    np.save(Config.INDEX_DIR / f\"{prefix}_embeddings.npy\", embeddings)\n    with open(Config.INDEX_DIR / f\"{prefix}_metadata.json\", \"w\") as f:\n        json.dump({'paths': paths, 'labels': labels}, f)\n\n\ndef load_index_data(prefix=\"test\"):\n    emb = np.load(Config.INDEX_DIR / f\"{prefix}_embeddings.npy\")\n    with open(Config.INDEX_DIR / f\"{prefix}_metadata.json\") as f:\n        meta = json.load(f)\n    return emb, meta['paths'], meta['labels']\n\n\ndef build_faiss_index(embeddings):\n    dim = embeddings.shape[1]\n    if Config.FAISS_TYPE == \"ivf\" and len(embeddings) > 1000:\n        nlist = min(100, len(embeddings)//10)\n        quant = faiss.IndexFlatIP(dim)\n        index = faiss.IndexIVFFlat(quant, dim, nlist, faiss.METRIC_INNER_PRODUCT)\n        index.train(embeddings)\n        index.nprobe = 10\n    else:\n        index = faiss.IndexFlatIP(dim)\n    index.add(embeddings)\n    print(f\"FAISS index built: {index.ntotal} vectors\")\n    return index\n\n\ndef save_faiss_index(index, name=\"test_index.faiss\"):\n    faiss.write_index(index, str(Config.INDEX_DIR / name))\n\n\ndef load_faiss_index(name=\"test_index.faiss\"):\n    idx = faiss.read_index(str(Config.INDEX_DIR / name))\n    print(f\"FAISS index loaded: {idx.ntotal} vectors\")\n    return idx\n\n\n# ─────────────────────────────────────────────\n#  RETRIEVAL SYSTEM\n# ─────────────────────────────────────────────\nclass RetrievalSystem:\n    def __init__(self, index, embedder, db_paths, db_labels):\n        self.index = index\n        self.embedder = embedder\n        self.db_paths = db_paths\n        self.db_labels = db_labels\n\n    def image_to_image(self, query_path: str, k: int = 5) -> List[Dict]:\n        img = Image.open(query_path).convert(\"RGB\")\n        q_emb = self.embedder.encode_image([img]).astype(np.float32)\n        scores, indices = self.index.search(q_emb, k + 1)\n        results = []\n        for sc, idx in zip(scores[0], indices[0]):\n            if self.db_paths[idx] == query_path:\n                continue\n            results.append({\n                'path': self.db_paths[idx],\n                'label': Config.CLASSES[self.db_labels[idx]],\n                'score': float(sc),\n            })\n        return results[:k]\n\n    def text_to_image(self, query_text: str, k: int = 5) -> List[Dict]:\n        if not hasattr(self.embedder, 'encode_text'):\n            raise AttributeError(\"Embedder does not support text queries.\")\n        q_emb = self.embedder.encode_text([query_text]).astype(np.float32)\n        scores, indices = self.index.search(q_emb, k)\n        return [{\n            'path': self.db_paths[i],\n            'label': Config.CLASSES[self.db_labels[i]],\n            'score': float(s),\n        } for s, i in zip(scores[0], indices[0])]\n\n\n# ─────────────────────────────────────────────\n#  EVALUATION\n# ─────────────────────────────────────────────\ndef evaluate_precision_at_k(rs: RetrievalSystem, queries: List[Dict], k_values: List[int]):\n    results = {k: [] for k in k_values}\n    maxk = max(k_values)\n    for rec in tqdm(queries, desc=\"P@k eval\"):\n        lbl = rec['label_idx']\n        ret = rs.image_to_image(str(rec['path']), maxk)\n        for kk in k_values:\n            top = ret[:kk]\n            correct = sum(1 for x in top if Config.CLASSES.index(x['label']) == lbl)\n            results[kk].append(correct / kk)\n    df = pd.DataFrame({\n        f'P@{k}': [np.mean(results[k]), np.std(results[k])]\n        for k in k_values\n    }, index=['mean', 'std'])\n    print(\"\\nPrecision@k:\\n\", df.round(4))\n    df.to_csv(Config.OUTPUT_DIR / \"precision_at_k.csv\")\n    return df\n\n\n# ─────────────────────────────────────────────\n#  MAIN\n# ─────────────────────────────────────────────\ndef main(mode=\"full\", query=None, k=5):\n    random.seed(Config.SEED)\n    np.random.seed(Config.SEED)\n    setup_dirs()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Device: {device} | Mode: {mode}\")\n\n    embedder = get_embedder(device)\n\n    if mode in (\"search_image\", \"search_text\") and query:\n        emb, paths, lbls = load_index_data(\"test\")\n        idx = load_faiss_index()\n        rs = RetrievalSystem(idx, embedder, paths, lbls)\n        if mode == \"search_image\":\n            print(f\"\\nImage search → {query}\")\n            for i, r in enumerate(rs.image_to_image(query, k), 1):\n                print(f\"  #{i} | {r['label']:<10} | score={r['score']:.4f} | {r['path']}\")\n        else:\n            print(f\"\\nText search → {query}\")\n            for i, r in enumerate(rs.text_to_image(query, k), 1):\n                print(f\"  #{i} | {r['label']:<10} | score={r['score']:.4f} | {r['path']}\")\n        return\n\n    print(\"\\nLoading test set …\")\n    test_records = load_all_images(\"test\")\n\n    print(\"\\nExtracting embeddings …\")\n    embeddings, paths, labels = extract_embeddings(test_records, embedder)\n    save_index_data(embeddings, paths, labels, \"test\")\n\n    print(\"\\nBuilding FAISS index …\")\n    index = build_faiss_index(embeddings)\n    save_faiss_index(index)\n\n    rs = RetrievalSystem(index, embedder, paths, labels)\n\n    if mode in (\"evaluate\", \"full\"):\n        print(\"\\nEvaluating …\")\n        evaluate_precision_at_k(rs, test_records, Config.TOP_K)\n\n    print(\"\\nTask 3 finished.\")\n    print(f\"Outputs → {Config.OUTPUT_DIR}\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"Task 3: CBIR for Chest X-rays\")\n    parser.add_argument(\"--mode\", default=\"full\",\n                        choices=[\"full\", \"build\", \"evaluate\", \"search_image\", \"search_text\"])\n    parser.add_argument(\"--query\", default=None, type=str)\n    parser.add_argument(\"--k\", default=5, type=int)\n\n    # ── This line fixes the Jupyter/Kaggle -f kernel.json error ──\n    args, unknown = parser.parse_known_args()\n\n    if unknown:\n        print(\"Ignored extra arguments (normal in Jupyter/Kaggle):\", unknown)\n\n    main(mode=args.mode, query=args.query, k=args.k)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T15:22:13.142300Z","iopub.execute_input":"2026-02-18T15:22:13.142632Z","iopub.status.idle":"2026-02-18T15:22:46.911537Z","shell.execute_reply.started":"2026-02-18T15:22:13.142606Z","shell.execute_reply":"2026-02-18T15:22:46.910586Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\nRequirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (26.0rc2)\nIgnored extra arguments (normal in Jupyter/Kaggle): ['-f', '/root/.local/share/jupyter/runtime/kernel-700b7ee0-80f0-4a3c-ab2e-1a272e333c82.json']\nDevice: cuda | Mode: full\nCLIP load failed: No module named 'clip'\nFalling back to ResNet (no text support)\n\nLoading test set …\n[TEST] Loaded 624 images\n\nExtracting embeddings …\n","output_type":"stream"},{"name":"stderr","text":"Extracting: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it]\n","output_type":"stream"},{"name":"stdout","text":"Embeddings shape: (624, 2048)\n\nBuilding FAISS index …\nFAISS index built: 624 vectors\n\nEvaluating …\n","output_type":"stream"},{"name":"stderr","text":"P@k eval: 100%|██████████| 624/624 [00:16<00:00, 37.77it/s]","output_type":"stream"},{"name":"stdout","text":"\nPrecision@k:\n          P@1     P@3     P@5    P@10\nmean  0.8702  0.8494  0.8378  0.8306\nstd   0.3361  0.2637  0.2454  0.2273\n\nTask 3 finished.\nOutputs → /kaggle/working/task3_outputs\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13}]}